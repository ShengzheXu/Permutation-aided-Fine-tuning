{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_time = np.array([182.87606406211853, 163.16899490356445, 160.77447295188904, 169.1652810573578, 161.25752568244934])\n",
    "sample_time = np.array([6.197754621505737, 5.553525447845459, 5.609967231750488, 5.5619001388549805, 5.4711527824401855])\n",
    "\n",
    "print(\"CTGAN method:\")\n",
    "print(f\" - Train time: {(train_time.mean()/60).round(2)} min.\")\n",
    "print(f\" - Sampling time: {sample_time.mean().round(2)} sec.\")\n",
    "\n",
    "train_time = np.array([342.2909104824066, 342.98024106025696, 342.817191362381, 344.6774263381958, 344.17060708999634])\n",
    "sample_time = np.array([1.0080089569091797, 1.030515193939209, 1.0197803974151611, 1.0102648735046387, 1.0290708541870117])\n",
    "\n",
    "print(\"\\nCopulaGAN method:\")\n",
    "print(f\" - Train time: {(train_time.mean()/60).round(2)} min.\")\n",
    "print(f\" - Sampling time: {sample_time.mean().round(2)} sec.\")\n",
    "\n",
    "vae_train_time = np.array([26.7655]) # min\n",
    "train_time = np.array([390.08692240715027])\n",
    "sample_time = np.array([4.811263084411621, 4.804065704345703, 4.816697359085083, 4.792133092880249, 4.804851770401001])\n",
    "\n",
    "print(\"\\nTabSyn method:\")\n",
    "print(f\" - Train time: {(vae_train_time+(train_time/60)).round(2)[0]} min.\")\n",
    "print(f\" - Sampling time: {sample_time.mean().round(2)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_cat_cols(df):\n",
    "  label_dict = {}\n",
    "  for col in df.columns:\n",
    "    sample = np.nan\n",
    "    for i in range(len(df[col])):\n",
    "      if df[col][i]!=df[col][i]: continue\n",
    "      else:\n",
    "        sample = df[col][i]\n",
    "        break\n",
    "    if isinstance(sample, str):\n",
    "      label_dict[col] = {}\n",
    "      le = LabelEncoder()\n",
    "      le.fit(list(df[col]))\n",
    "      labels = list(le.classes_)\n",
    "      for i in range(len(labels)):\n",
    "        label_dict[col][labels[i]] = i\n",
    "\n",
    "  return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"california_housing\"\n",
    "\n",
    "train = pd.read_csv(f'../data_to_eval/train/{filename}.csv') # train dataset\n",
    "\n",
    "categorical_columns = []\n",
    "for col in list(train.columns):\n",
    "  sample = np.nan\n",
    "  for i in range(len(train[col])):\n",
    "    if train[col][i]!=train[col][i]: continue\n",
    "    else:\n",
    "      sample = train[col][i]\n",
    "      break\n",
    "  if isinstance(sample, str):\n",
    "    categorical_columns.append(col)\n",
    "label_dictionary = encode_cat_cols(train)\n",
    "train = train.dropna().reset_index(drop=True)\n",
    "\n",
    "test = pd.read_csv(f'../data_to_eval/test/{filename}.csv') # test dataset\n",
    "test = test.dropna().reset_index(drop=True)\n",
    "\n",
    "ctgan = []\n",
    "for i in range(5):\n",
    "  df = pd.read_csv(f'../data_to_eval/ctgan/{filename}_{i}.csv')\n",
    "  df = df.dropna().reset_index(drop=True)\n",
    "  ctgan.append(df)\n",
    "\n",
    "copulagan = []\n",
    "for i in range(5):\n",
    "  df = pd.read_csv(f'../data_to_eval/copulagan/{filename}_{i}.csv')\n",
    "  df = df.dropna().reset_index(drop=True)\n",
    "  copulagan.append(df)\n",
    "\n",
    "tabsyn = []\n",
    "for i in range(5):\n",
    "  df = pd.read_csv(f'../data_to_eval/tabsyn/{filename}_{i}.csv')\n",
    "  df = df.dropna().reset_index(drop=True)\n",
    "  tabsyn.append(df)\n",
    "\n",
    "great = []\n",
    "for i in range(5):\n",
    "  df = pd.read_csv(f'../data_to_eval/great/{filename}_{i}.csv')\n",
    "  df = df.dropna().reset_index(drop=True)\n",
    "  great.append(df)\n",
    "\n",
    "paft = []\n",
    "for i in range(5):\n",
    "  df = pd.read_csv(f'../data_to_eval/paft/{filename}_{i}.csv')\n",
    "  df = df.dropna().reset_index(drop=True)\n",
    "  paft.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCTGAN dataset\")\n",
    "total_lack = np.array([0]*5)\n",
    "total_col_involved = np.array([0]*5)\n",
    "for i in range(5):\n",
    "  for col in label_dictionary.keys():\n",
    "    lack = list(set(label_dictionary[col].keys()- set(ctgan[i][col])))\n",
    "    if len(lack):\n",
    "      total_col_involved[i] += 1\n",
    "    total_lack[i] += len(lack)\n",
    "print(f\"=> Total missed: {int(np.median(total_lack))} values, {int(np.median(total_col_involved))} columns involved.\")\n",
    "\n",
    "print(\"\\nCopulaGAN dataset\")\n",
    "total_lack = np.array([0]*5)\n",
    "total_col_involved = np.array([0]*5)\n",
    "for i in range(5):\n",
    "  for col in label_dictionary.keys():\n",
    "    lack = list(set(label_dictionary[col].keys()- set(copulagan[i][col])))\n",
    "    if len(lack):\n",
    "      total_col_involved[i] += 1\n",
    "    total_lack[i] += len(lack)\n",
    "print(f\"=> Total missed: {int(np.median(total_lack))} values, {int(np.median(total_col_involved))} columns involved.\")\n",
    "\n",
    "print(\"\\nTabSyn dataset\")\n",
    "total_lack = np.array([0]*5)\n",
    "total_col_involved = np.array([0]*5)\n",
    "for i in range(5):\n",
    "  for col in label_dictionary.keys():\n",
    "    lack = list(set(label_dictionary[col].keys()- set(tabsyn[i][col])))\n",
    "    if len(lack):\n",
    "      total_col_involved[i] += 1\n",
    "    total_lack[i] += len(lack)\n",
    "print(f\"=> Total missed: {int(np.median(total_lack))} values, {int(np.median(total_col_involved))} columns involved.\")\n",
    "\n",
    "print(\"\\nGreat dataset\")\n",
    "total_lack = np.array([0]*5)\n",
    "total_col_involved = np.array([0]*5)\n",
    "for i in range(5):\n",
    "  for col in label_dictionary.keys():\n",
    "    lack = list(set(label_dictionary[col].keys()- set(great[i][col])))\n",
    "    if len(lack):\n",
    "      total_col_involved[i] += 1\n",
    "    total_lack[i] += len(lack)\n",
    "print(f\"=> Total missed: {int(np.median(total_lack))} values, {int(np.median(total_col_involved))} columns involved.\")\n",
    "\n",
    "print(\"\\nPAFT dataset\")\n",
    "total_lack = np.array([0]*5)\n",
    "total_col_involved = np.array([0]*5)\n",
    "for i in range(5):\n",
    "  for col in label_dictionary.keys():\n",
    "    lack = list(set(label_dictionary[col].keys()- set(paft[i][col])))\n",
    "    if len(lack):\n",
    "      total_col_involved[i] += 1\n",
    "    total_lack[i] += len(lack)\n",
    "print(f\"=> Total missed: {int(np.median(total_lack))} values, {int(np.median(total_col_involved))} columns involved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge check: median housing price should be 14999-500001\n",
    "print(\"Knowledge check: median housing price should be 14999-500001\")\n",
    "\n",
    "wrong = [0]*5\n",
    "print(\"\\nCTGAN dataset\")\n",
    "for i in range(5):\n",
    "  for j in range(len(ctgan[i])):\n",
    "    if ctgan[i]['median_house_value'][j]<14999 or ctgan[i]['median_house_value'][j]>500001:\n",
    "      wrong[i] += 1\n",
    "wrong = np.array(wrong)/len(ctgan[i])*100\n",
    "print(\"\\n - CTGAN method:\")\n",
    "print(f\"    -> Error: {np.mean(wrong).round(2)} % (+/-{np.std(wrong).round(2)})\")\n",
    "\n",
    "wrong = [0]*5\n",
    "print(\"\\nCopulaGAN dataset\")\n",
    "for i in range(5):\n",
    "  for j in range(len(copulagan[i])):\n",
    "    if copulagan[i]['median_house_value'][j]<14999 or copulagan[i]['median_house_value'][j]>500001:\n",
    "      wrong[i] += 1\n",
    "wrong = np.array(wrong)/len(copulagan[i])*100\n",
    "print(\"\\n - CopulaGAN method:\")\n",
    "print(f\"    -> Error: {np.mean(wrong).round(2)} % (+/-{np.std(wrong).round(2)})\")\n",
    "\n",
    "wrong = [0]*5\n",
    "print(\"\\nTabSyn dataset\")\n",
    "for i in range(5):\n",
    "  for j in range(len(tabsyn[i])):\n",
    "    if tabsyn[i]['median_house_value'][j]<14999 or tabsyn[i]['median_house_value'][j]>500001:\n",
    "      wrong[i] += 1\n",
    "wrong = np.array(wrong)/len(tabsyn[i])*100\n",
    "print(f\"    -> Error: {np.mean(wrong).round(2)} % (+/-{np.std(wrong).round(2)})\")\n",
    "\n",
    "wrong = [0]*5\n",
    "print(\"\\n - GReaT method:\")\n",
    "for i in range(5):\n",
    "  for j in range(len(great[i])):\n",
    "    if great[i]['median_house_value'][j]<14999 or great[i]['median_house_value'][j]>500001:\n",
    "      wrong[i] += 1\n",
    "wrong = np.array(wrong)/len(great[i])*100\n",
    "print(f\"    -> Error: {np.mean(wrong).round(2)} % (+/-{np.std(wrong).round(2)})\")\n",
    "\n",
    "wrong = [0]*5\n",
    "print(\"\\nPAFT dataset\")\n",
    "for i in range(5):\n",
    "  for j in range(len(paft[i])):\n",
    "    if paft[i]['median_house_value'][j]<14999 or paft[i]['median_house_value'][j]>500001:\n",
    "      wrong[i] += 1\n",
    "wrong = np.array(wrong)/len(paft[i])*100\n",
    "print(\"\\n - PAFT method:\")\n",
    "print(f\"    -> Error: {np.mean(wrong).round(2)} % (+/-{np.std(wrong).round(2)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "\n",
    "for i in range(5):\n",
    "  ctgan[i] = ctgan[i][list(train.columns)] # re-order columns\n",
    "  ctgan[i].replace(label_dictionary, inplace=True)\n",
    "\n",
    "  copulagan[i] = copulagan[i][list(train.columns)] # re-order columns\n",
    "  copulagan[i].replace(label_dictionary, inplace=True)\n",
    "\n",
    "  tabsyn[i] = tabsyn[i][list(train.columns)] # re-order columns\n",
    "  tabsyn[i].replace(label_dictionary, inplace=True)\n",
    "\n",
    "  great[i] = great[i][list(train.columns)] # re-order columns\n",
    "  great[i].replace(label_dictionary, inplace=True)\n",
    "\n",
    "  paft[i] = paft[i][list(train.columns)] # re-order columns\n",
    "  paft[i].replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - ML Efficieny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with generated data and test with ground truth\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_percentage_error\n",
    "\n",
    "def MLE(train_df, test_df, label_col, label_col_discrete):\n",
    "  X_train = []\n",
    "  y_train = []\n",
    "  X_test = []\n",
    "  y_test = []\n",
    "  for i in range(len(train_df)):\n",
    "    df = train_df[i].copy().drop([label_col], axis=1)\n",
    "    X_train.append(df)\n",
    "    y_train.append(train_df[i][label_col])\n",
    "\n",
    "    df = test_df.copy().drop([label_col], axis=1)\n",
    "    X_test.append(df)\n",
    "    y_test.append(test_df[label_col])\n",
    "\n",
    "  if label_col_discrete:\n",
    "    RF = RandomForestClassifier()\n",
    "    accuracy = []\n",
    "    for i in range(len(train_df)):\n",
    "      RF.fit(X_train[i], y_train[i])\n",
    "      y_pred = RF.predict(X_test[i])\n",
    "      accuracy.append(accuracy_score(y_test[i], y_pred))\n",
    "    accuracy = np.array(accuracy)*100\n",
    "    print(f'  -> RF accuracy: {np.mean(accuracy).round(2)} % (+/-{np.std(accuracy).round(2)})')\n",
    "    \n",
    "    LR = LogisticRegression()\n",
    "    accuracy = []\n",
    "    for i in range(len(train_df)):\n",
    "      LR.fit(X_train[i], y_train[i])\n",
    "      y_pred = LR.predict(X_test[i])\n",
    "      accuracy.append(accuracy_score(y_test[i], y_pred))\n",
    "    accuracy = np.array(accuracy)*100\n",
    "    print(f'  -> LR accuracy: {np.mean(accuracy).round(2)} % (+/-{np.std(accuracy).round(2)})')\n",
    "\n",
    "    NN = MLPClassifier(solver='adam', hidden_layer_sizes=(150, 100, 50), max_iter=300, activation='relu')\n",
    "    accuracy = []\n",
    "    for i in range(len(train_df)):\n",
    "      NN.fit(X_train[i], y_train[i])\n",
    "      y_pred = NN.predict(X_test[i])\n",
    "      accuracy.append(accuracy_score(y_test[i], y_pred))\n",
    "    accuracy = np.array(accuracy)*100\n",
    "    print(f'  -> NN accuracy: {np.mean(accuracy).round(2)} % (+/-{np.std(accuracy).round(2)})')\n",
    "  else:\n",
    "    RF = RandomForestRegressor()\n",
    "    mape = []\n",
    "    for i in range(len(train_df)):\n",
    "      RF.fit(X_train[i], y_train[i])\n",
    "      y_pred = RF.predict(X_test[i])\n",
    "      mape.append(mean_absolute_percentage_error(y_test[i], y_pred))\n",
    "    mape = np.array(mape)\n",
    "    print(f'  -> RF mape: {np.mean(mape).round(2)} % (+/-{np.std(mape).round(2)})')\n",
    "\n",
    "    LR = LinearRegression()\n",
    "    mape = []\n",
    "    for i in range(len(train_df)):\n",
    "      LR.fit(X_train[i], y_train[i])\n",
    "      y_pred = LR.predict(X_test[i])\n",
    "      mape.append(mean_absolute_percentage_error(y_test[i], y_pred))\n",
    "    mape = np.array(mape)\n",
    "    print(f'  -> LR mape: {np.mean(mape).round(2)} % (+/-{np.std(mape).round(2)})')\n",
    "\n",
    "    NN = MLPRegressor(solver='adam', hidden_layer_sizes=(150, 100, 50), max_iter=300, activation='relu')\n",
    "    mape = []\n",
    "    for i in range(len(train_df)):\n",
    "      NN.fit(X_train[i], y_train[i])\n",
    "      y_pred = NN.predict(X_test[i])\n",
    "      mape.append(mean_absolute_percentage_error(y_test[i], y_pred))\n",
    "    mape = np.array(mape)\n",
    "    print(f\"  -> NN mape: {np.mean(mape).round(2)} % (+/-{np.std(mape).round(2)})\")\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "\n",
    "print(\"\\n - Original data:\")\n",
    "MLE([train], test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\"\\n - CTGAN method:\")\n",
    "MLE(ctgan, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\"\\n - CopulaGAN method:\")\n",
    "MLE(copulagan, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\"\\n - TabSyn method:\")\n",
    "MLE(tabsyn, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\"\\n - GReaT method:\")\n",
    "MLE(great, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\"\\n - PAFT method:\")\n",
    "MLE(paft, test, 'median_house_value', label_col_discrete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with ground truth + random data (as different as possible), then test generated data to see if its real/fake\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def Discriminator(train_df, test_df, generated_df):\n",
    "  train_data = train_df.copy()\n",
    "  train_data['real'] = 1\n",
    "  random = {}\n",
    "  float_cols = []\n",
    "  for col in list(train_data.columns):\n",
    "    random[col] = np.random.choice(train_data[col], len(train_data))\n",
    "  random = pd.DataFrame(random)\n",
    "  for col in float_cols:\n",
    "    random[col] = random[col].astype(float)\n",
    "  random['real'] = 0\n",
    "  X_train = shuffle(pd.concat([train_data, random])).reset_index(drop=True)\n",
    "  y_train = X_train['real']\n",
    "  X_train = X_train.drop(['real'], axis=1)\n",
    "  \n",
    "  X_test = []\n",
    "  y_test = []\n",
    "  for i in range(5):\n",
    "    generated_data = generated_df[i].copy()\n",
    "    generated_data['real'] = 0\n",
    "    test_data = test_df.copy()\n",
    "    test_data['real'] = 1\n",
    "    generated_data = generated_data.sample(len(test_data))\n",
    "    X_test.append(shuffle(pd.concat([generated_data, test_data])).reset_index(drop=True))\n",
    "    y_test.append(X_test[i]['real'])\n",
    "    X_test[i] = X_test[i].drop(['real'], axis=1)\n",
    "\n",
    "  RF = RandomForestClassifier()\n",
    "  accuracy = []\n",
    "  for i in range(5):\n",
    "    RF.fit(X_train, y_train)\n",
    "    y_pred = RF.predict(X_test[i])\n",
    "    accuracy.append(accuracy_score(y_test[i], y_pred))\n",
    "  accuracy = np.array(accuracy)*100\n",
    "  print(f'    -> RF accuracy: {np.mean(accuracy).round(2)} % (+/-{np.std(accuracy).round(2)})')\n",
    "\n",
    "print(\"\\nDiscriminator (Training with Real + Random data. Closer to 50% accuracy is better):\")\n",
    "\n",
    "print(\"\\n - CTGAN method:\")\n",
    "Discriminator(train, test, ctgan)\n",
    "\n",
    "print(\"\\n - CopulaGAN method:\")\n",
    "Discriminator(train, test, copulagan)\n",
    "\n",
    "print(\"\\n - TabSyn method:\")\n",
    "Discriminator(train, test, tabsyn)\n",
    "\n",
    "print(\"\\n - GReaT method:\")\n",
    "Discriminator(train, test, great)\n",
    "\n",
    "print(\"\\n - PAFT method:\")\n",
    "Discriminator(train, test, paft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution (single/multi-variate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'../data_to_eval/train/{filename}.csv') # train dataset\n",
    "train = train.dropna().reset_index(drop=True)\n",
    "\n",
    "ctgan_df = []\n",
    "copulagan_df = []\n",
    "tabsyn_df = []\n",
    "great_df = []\n",
    "paft_df = []\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f'../data_to_eval/ctgan/{filename}_{i}.csv')\n",
    "    df = df[list(train.columns)] # re-order columns\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    ctgan_df.append(df)\n",
    "\n",
    "    df = pd.read_csv(f'../data_to_eval/copulagan/{filename}_{i}.csv')\n",
    "    df = df[list(train.columns)] # re-order columns\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    copulagan_df.append(df)\n",
    "\n",
    "    df = pd.read_csv(f'../data_to_eval/tabsyn/{filename}_{i}.csv')\n",
    "    df = df[list(train.columns)] # re-order columns\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    tabsyn_df.append(df)\n",
    "\n",
    "    df = pd.read_csv(f'../data_to_eval/great/{filename}_{i}.csv')\n",
    "    df = df[list(train.columns)] # re-order columns\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    great_df.append(df)\n",
    "\n",
    "    df = pd.read_csv(f'../data_to_eval/paft/{filename}_{i}.csv')\n",
    "    df = df[list(train.columns)] # re-order columns\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    paft_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "def distribution(train, col):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(30, 5))\n",
    "    \n",
    "    train_data = train_df.copy()\n",
    "    train_data[\"method\"] = \"Real\"\n",
    "    for i in range(5):\n",
    "        ctgan_data = ctgan_df[i].copy()\n",
    "        copulagan_data = copulagan_df[i].copy()\n",
    "        tabsyn_data = tabsyn_df[i].copy()\n",
    "        great_data = great_df[i].copy()\n",
    "        paft_data = paft_df[i].copy()\n",
    "\n",
    "        ctgan_data[\"method\"] = \"CTGAN\"\n",
    "        copulagan_data[\"method\"] = \"CopulaGAN\"\n",
    "        tabsyn_data[\"method\"] = \"TabSyn\"\n",
    "        great_data[\"method\"] = \"GReaT\"\n",
    "        paft_data[\"method\"] = \"PAFT\"\n",
    "        df = pd.concat([train_data, ctgan_data, copulagan_data, tabsyn_data, great_data, paft_data]).reset_index(drop=True)\n",
    "\n",
    "        if col in categorical_columns:\n",
    "            sns.countplot(data=df, x=col, hue=\"method\", ax=axes[i], palette=['r', '#b9f2f0', '#d0bbff', '#8de5a1', '#FFE48F', 'b'])\n",
    "            axes[i].tick_params(axis='x', rotation=90)\n",
    "        else:\n",
    "            sns.kdeplot(data=df, x=col, hue=\"method\", ax=axes[i], shade=True, palette=['r', '#b9f2f0', '#d0bbff', '#8de5a1', '#FFE48F', 'b'])\n",
    "    \n",
    "    plt.savefig(f'./distribution/{filename}_{col}.png')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nDistribution:\")\n",
    "\n",
    "for col in list(train.columns):\n",
    "  print(f\" - {col} column:\")\n",
    "  distribution(train, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "def distribution(train, columns, indexes):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(7, 15))\n",
    "    \n",
    "    train_data = train_df.copy()\n",
    "    train_data[\"method\"] = \"Real\"\n",
    "    \n",
    "    for col, i in zip(columns, indexes):\n",
    "        ctgan_data = ctgan_df[i].copy()\n",
    "        copulagan_data = copulagan_df[i].copy()\n",
    "        tabsyn_data = tabsyn_df[i].copy()\n",
    "        great_data = great_df[i].copy()\n",
    "        paft_data = paft_df[i].copy()\n",
    "\n",
    "        ctgan_data[\"method\"] = \"CTGAN\"\n",
    "        copulagan_data[\"method\"] = \"CopulaGAN\"\n",
    "        tabsyn_data[\"method\"] = \"TabSyn\"\n",
    "        great_data[\"method\"] = \"GReaT\"\n",
    "        paft_data[\"method\"] = \"PAFT\"\n",
    "        df = pd.concat([train_data, ctgan_data, copulagan_data, tabsyn_data, great_data, paft_data]).reset_index(drop=True)\n",
    "\n",
    "        if col in categorical_columns:\n",
    "            sns.countplot(data=df, x=col, hue=\"method\", ax=axes[1], palette=['r', '#b9f2f0', '#d0bbff', '#8de5a1', '#FFE48F', 'b'])\n",
    "            axes[1].set_xlabel(col, fontsize=20)\n",
    "            axes[1].set_ylabel('# Count', fontsize=20)\n",
    "            axes[1].legend(loc='upper right')\n",
    "        else:\n",
    "            sns.kdeplot(data=df, x=col, hue=\"method\", ax=axes[0], palette=['r', '#b9f2f0', '#d0bbff', '#8de5a1', '#FFE48F', 'b'], fill=True)\n",
    "            axes[0].set_xlabel(col, fontsize=20)\n",
    "            axes[0].set_ylabel('Density', fontsize=20)\n",
    "            axes[0].set_title('California', fontsize=24)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nDistribution:\")\n",
    "\n",
    "columns = ['ocean_proximity', 'median_house_value']\n",
    "distribution(train, columns, [2, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_gpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
